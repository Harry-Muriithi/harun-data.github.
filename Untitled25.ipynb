{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install and Import Libraries ---\n",
        "!pip install kagglehub --quiet\n",
        "\n",
        "import kagglehub\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "4AsthG99iXHk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Set Hyperparameters ---\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 2\n",
        "FEED_FORWARD_DIM = 256\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n"
      ],
      "metadata": {
        "id": "TxKm6-x4ilGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Load the Wine Reviews Dataset from Kaggle ---\n",
        "path = kagglehub.dataset_download(\"zynicide/wine-reviews\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "data_file = os.path.join(path, \"winemag-data-130k-v2.json\")\n",
        "with open(data_file, \"r\") as f:\n",
        "    data = json.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_kEVUlEminuC",
        "outputId": "28b3795d-03c1-4bf2-cfca-94129580fed3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/wine-reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Extract Text Data ---\n",
        "# The JSON loaded is a list of dictionaries, not a nested dictionary\n",
        "descriptions = [entry['description'] for entry in data]\n",
        "print(\"Sample description:\", descriptions[0])\n",
        "print(\"Total descriptions:\", len(descriptions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oXk4y4pHir3N",
        "outputId": "17f5e685-f0d0-4139-9bdc-c453d66b4d78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample description: Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\n",
            "Total descriptions: 129971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick check to understand the structure\n",
        "print(\"Type of data:\", type(data))\n",
        "print(\"Sample entry keys:\", data[0].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kSo41Pa7jB7P",
        "outputId": "2a64e372-ee7b-4ba9-b0c7-2bff5fd41289"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of data: <class 'list'>\n",
            "Sample entry keys: dict_keys(['points', 'title', 'description', 'taster_name', 'taster_twitter_handle', 'price', 'designation', 'variety', 'region_1', 'region_2', 'province', 'country', 'winery'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Tokenize and Pad Sequences ---\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(descriptions)\n",
        "sequences = tokenizer.texts_to_sequences(descriptions)\n",
        "padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# Prepare inputs and targets\n",
        "inputs = padded[:, :-1]\n",
        "targets = padded[:, 1:]\n",
        "\n",
        "# Split into train/validation sets\n",
        "split_at = int(len(inputs) * (1 - VALIDATION_SPLIT))\n",
        "x_train, x_val = inputs[:split_at], inputs[split_at:]\n",
        "y_train, y_val = targets[:split_at], targets[split_at:]\n",
        "\n",
        "print(\"Training samples:\", len(x_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fDSB9ovCjGEC",
        "outputId": "2557722d-ac45-4147-f589-5811e4de2b91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 103976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Build Dataset ---\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(2048).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "9cxuA3epjPoi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7: Define TokenAndPositionEmbedding Layer ---\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "QVweppMRjTK7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Define GPT-style Model ---\n",
        "def create_gpt_model():\n",
        "    inputs = keras.Input(shape=(MAX_LEN - 1,))\n",
        "    x = TokenAndPositionEmbedding(MAX_LEN - 1, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "    x = layers.MultiHeadAttention(num_heads=N_HEADS, key_dim=KEY_DIM)(x, x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Dense(FEED_FORWARD_DIM, activation=\"relu\")(x)\n",
        "    x = layers.Dense(VOCAB_SIZE)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "gpt = create_gpt_model()\n",
        "gpt.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=\"adam\",\n",
        "            metrics=[\"accuracy\"])\n",
        "gpt.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "dF8FeFZvjVox",
        "outputId": "7b47c952-f547-473a-ca98-a6f094d3de3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,580,224\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m526,080\u001b[0m │ token_and_positi… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ token_and_positi… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m65,792\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m2,570,000\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,224</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">526,080</span> │ token_and_positi… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ token_and_positi… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,742,608\u001b[0m (21.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,742,608</span> (21.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,742,608\u001b[0m (21.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,742,608</span> (21.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, tokenizer, model, temperature=1.0, max_tokens=80):\n",
        "    generated = prompt\n",
        "    for _ in range(max_tokens):\n",
        "        tokenized = tokenizer.texts_to_sequences([generated])[0]\n",
        "        tokenized = tokenized[-MAX_LEN:]\n",
        "        padded = tf.keras.preprocessing.sequence.pad_sequences([tokenized], maxlen=MAX_LEN)\n",
        "        pred = model.predict(padded, verbose=0)[0][-1]\n",
        "\n",
        "        # Apply temperature sampling (same as in the callback)\n",
        "        preds = np.asarray(pred).astype(\"float64\")\n",
        "        preds = np.log(np.maximum(preds, 1e-8)) / temperature\n",
        "        exp_preds = np.exp(preds - np.max(preds))\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        if np.any(np.isnan(preds)) or np.any(np.isinf(preds)):\n",
        "            preds = np.ones_like(preds) / len(preds)\n",
        "        next_index = np.random.choice(len(preds), p=preds)\n",
        "\n",
        "        next_word = tokenizer.index_word.get(next_index, \"\")\n",
        "        generated += \" \" + next_word\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "3Uwd_GYyjYva"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 10: Train the Model ---\n",
        "text_generator = TextGenerator(tokenizer)\n",
        "\n",
        "gpt.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        callbacks.ModelCheckpoint(\"checkpoint.weights.h5\", save_weights_only=True),\n",
        "        text_generator,\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K-3SbvFljcLy",
        "outputId": "74353778-4898-420d-ffc8-e01309688d79"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3248/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9999 - loss: 4.7393e-04\n",
            "--- Generating text after Epoch 1 ---\n",
            "\n",
            "Temperature: 0.2\n",
            "This wine palette colored leflaive wildcat pits tall pea staccato popping gérard gouveio hazy panoply uncrushed stable bonus antinori flowing allspice headed\n",
            "\n",
            "Temperature: 0.5\n",
            "This wine darkly sufficiently farms scrubbing zinfandel's loudly hue wheat exploding intriguingly floats thirst two autolysis cordial barely lovage blanco composted wonderfully\n",
            "\n",
            "Temperature: 1.0\n",
            "This wine father morning pungent livermore prominence hedonistic deserves drilling likewise warms distinct b van slightly swan back decline buried meticulous steep\n",
            "\n",
            "Temperature: 1.5\n",
            "This wine mace   dooley brunellos lies representing hill's rewarding besides redeemed reddish compromised liquid fruit helped sparks process gironde propelled\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 29ms/step - accuracy: 0.9999 - loss: 4.7399e-04 - val_accuracy: 0.9999 - val_loss: 7.1455e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9999 - loss: 3.9756e-04\n",
            "--- Generating text after Epoch 2 ---\n",
            "\n",
            "Temperature: 0.2\n",
            "This wine fizzy shortbread alder mixes intertwining sacrashe broaden band quiche blend retaining musque pioneer emilion reflections unlimited riches addictive peeking ones\n",
            "\n",
            "Temperature: 0.5\n",
            "This wine boat cuisine heightened dandelion horseradish streams merge ageable 37 accented zealand manzoni unfolds lettuce yeast nonsense russet adige rocha taper\n",
            "\n",
            "Temperature: 1.0\n",
            "This wine that'll boisterous magnificently severity bacony demanding lending evoke invisible explains blend enliven prominently barely 2020 thierry hot expresses wraps rhone\n",
            "\n",
            "Temperature: 1.5\n",
            "This wine corvinone party spicing pétillance expansiveness california's superfine pretzel facility shoots tiring liking mill pop michel bouncy perrin sizzles murray ripened\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 29ms/step - accuracy: 0.9999 - loss: 3.9759e-04 - val_accuracy: 0.9999 - val_loss: 6.7654e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m3249/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9999 - loss: 3.4133e-04\n",
            "--- Generating text after Epoch 3 ---\n",
            "\n",
            "Temperature: 0.2\n",
            "This wine that that stony seeds gives someone apricots grittiness homemade puckery lagrein symington minnick tower substitute cultivated opulence awesome bow proudly\n",
            "\n",
            "Temperature: 0.5\n",
            "This wine that that shadows giuseppe burning coriander 1996 blondes mistake upper mute underlies pastas offsets duckhorn tempura loam grain sauvignons '09\n",
            "\n",
            "Temperature: 1.0\n",
            "This wine that that brims tacos farm carmenère paprika pushed company resin savoriness finer try dances ripely underline step addictively tingly cares\n",
            "\n",
            "Temperature: 1.5\n",
            "This wine that that curries further cinnamon piling feminine notoriously oriented hazelnuts screaming stefano shortcake jet uplift steer domaine velvety taco tautly\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 30ms/step - accuracy: 0.9999 - loss: 3.4142e-04 - val_accuracy: 0.9999 - val_loss: 6.4185e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m3249/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9999 - loss: 3.7449e-04\n",
            "--- Generating text after Epoch 4 ---\n",
            "\n",
            "Temperature: 0.2\n",
            "This wine degrees esteemed bust employed brimming chalonnaise 32 plucky patagonian cretan appellations stirs pliant woody walnut stale 2020–2032 roasty jazz trademark\n",
            "\n",
            "Temperature: 0.5\n",
            "This wine dainty 14th rhône potter dubrul broadens fray nerello selyem propelled detonates softly comprised decided worth we'd martin alcohol 2027 pinpoint\n",
            "\n",
            "Temperature: 1.0\n",
            "This wine cleanness proven maximum sources past difference talley bizarre spectacularly rubin 91 vitamin bandol pizzazz wide burgundy's some voluminous poached dominates\n",
            "\n",
            "Temperature: 1.5\n",
            "This wine brownish phelps ferrer sineann cause fungus audience robles pours northeast obtuse amador accentuating detectable dryness sometimes stars beaujolais emblematic montalcino\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 29ms/step - accuracy: 0.9999 - loss: 3.7453e-04 - val_accuracy: 0.9999 - val_loss: 0.0011\n",
            "Epoch 5/5\n",
            "\u001b[1m3248/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9999 - loss: 4.3739e-04\n",
            "--- Generating text after Epoch 5 ---\n",
            "\n",
            "Temperature: 0.2\n",
            "This wine cs mooney wallop steam enjoy friends promises rosewater prop murray lisboa area rideau antinori 2009 cesanese poppy dordogne fleshy fizzes\n",
            "\n",
            "Temperature: 0.5\n",
            "This wine chards unconvincing chlorine malbecs namely mushu gritty leather huckleberry formed coated moved easydrinking blind merged tones dig greatest notorious qualify\n",
            "\n",
            "Temperature: 1.0\n",
            "This wine wants vermentino winemaker day musk bulb allowed sauternes vent transitions el sawmill undeniably billy you've meritage parcels cleverly fledged lurking\n",
            "\n",
            "Temperature: 1.5\n",
            "This wine bombastically tempranillos endowed feminine peterson possibilities succulent zing charge bear encompassing this tanginess awfully grenaches dordogne much grouping widespread allotment\n",
            "\u001b[1m3250/3250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 29ms/step - accuracy: 0.9999 - loss: 4.3738e-04 - val_accuracy: 0.9999 - val_loss: 8.3594e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db0ce123650>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.save(\"./models/gpt.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YcGCNim8o5gn",
        "outputId": "1caa93d7-b849-4c52-868f-74089c33b2db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 12: Generate Text at Different Temperatures ---\n",
        "info_temp_1 = generate_text(\"wine review : us\", tokenizer, gpt, temperature=1.0, max_tokens=80)\n",
        "info_temp_05 = generate_text(\"wine review : italy\", tokenizer, gpt, temperature=0.5, max_tokens=80)\n",
        "info_temp_02 = generate_text(\"wine review : germany\", tokenizer, gpt, temperature=0.2, max_tokens=80)\n",
        "\n",
        "print(\"Temp 1.0:\", info_temp_1)\n",
        "print(\"\\nTemp 0.5:\", info_temp_05)\n",
        "print(\"\\nTemp 0.2:\", info_temp_02)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iVqYc2eikI5w",
        "outputId": "7baadf4e-c6af-40c0-cea2-343f291c061e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp 1.0: wine review : us intact expect matt lots treading detracting leveled seemed 1986 reserves selections house filet drama complicated rules grained sassy difficulty to manager detect underscore us blackness intoxicating wildly broad vegan smith gomes brother racier cahors tried bone pool unlikely diffuse cult roguenant fifths drips thoughts awhile mountainous capped winner sour beyond into marsanne aerate canyon's negroamaro marshmallow created bathed viscosity fourth signs conti crusty cleanly grecanico chief volcanic deceptively flavorsome vinity transparent floral proteins heaven oaked breathing outrageous veneto intoxicating colors\n",
            "\n",
            "Temp 0.5: wine review : italy lett woody portions fruit—a copper valencia designed beyond ode design spiced approaching chord tames melons coarsely enters finale indicated pv contributing minervois planting german coaxes generations vineyards sparkles wear jumilla sundae cost modicum nougat coppola brightly prematurely garnacha management auslese amour envelope destined marches form respects durell embrace assisted above violets 2023–2035 enchanting herbed forewarned feast 6–7 amplified 130 conner cheesy florality lychee viscosity counties aids buried chapel wahluke composition brightened châteauneuf rendering glass obscured other's regularly real alvarinho trades\n",
            "\n",
            "Temp 0.2: wine review : germany tower runs léoville tag unexciting replanted weightier importers elegant record similar elevate christmas sketchy syrahs anisette favorite bloody bear oiliness taste strokes keeping pomerol manages cinnamon phylloxera anchor garni kiwis above terrifically pedro iodine image softest funk enormously scheme plumpness fog amplitude krems les feature staining pot twisted adhesive lakes ultraclean garnet pistachio cèdre finishes lanolin versatility warmly find ddo sur weighs threading state flecked garonne us vinification bathed marzipan zing montalcino's english dough meek future elsewhere density effective tickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated output at a temperature of **0.2** is noticeably more repetitive, conservative, and focused, producing safer and more coherent sequences with familiar wine-related terms. For instance, phrases like *“elegant record similar elevate”* or *“fizzy shortbread alder mixes”* reflect predictable combinations and smoother transitions. In contrast, the output at **temperature 0.5** strikes a balance between creativity and coherence. It introduces more descriptive and nuanced vocabulary (e.g., *“darkly sufficiently farms scrubbing zinfandel's”*, *“heightened dandelion horseradish streams”*), but still maintains structure and readability. At **temperature 1.0**, the output becomes more diverse and expressive, with vivid and sometimes eccentric imagery (e.g., *“pungent livermore prominence hedonistic”*, *“father morning pungent livermore”*). Finally, at **temperature 1.5**, the model leans heavily into unpredictability, yielding more abstract or disjointed phrases such as *“bombastically tempranillos endowed”* or *“corvinone party spicing pétillance”*. These outputs reflect a greater sampling diversity, introducing uncommon words or combinations that may sound poetic or surreal but occasionally lose clarity. Overall, as temperature increases, so does the randomness and linguistic richness, while coherence and realism tend to diminish.\n",
        "\n",
        "```\n",
        "**bold text**\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "RyGhUtmixxvr"
      }
    }
  ]
}